{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DNkuyu-RJdKW",
    "outputId": "c25a5b72-91c8-4e4c-f5d1-77b7b31f925d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "from imageio import imread\n",
    "# print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cityscapes/train/2189.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 256, 256, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = glob('cityscapes/train/*')\n",
    "tri = np.random.choice(path, size=1)\n",
    "print(tri[0])\n",
    "img = imread(tri[0])\n",
    "h, w, _ = img.shape\n",
    "# img.shape\n",
    "# _w\n",
    "_w = int(w/2)\n",
    "\n",
    "img_A, img_B = img[:, :_w, :], img[:, _w:, :]\n",
    "# img\n",
    "arr = np.array([[1,2,3,4],[5,6,7,8]])\n",
    "# img_A.shape\n",
    "# print(arr[:1,:])\n",
    "# print(arr[:])\n",
    "# arr.shape\n",
    "# arr = transform.resize(arr, (1,2))\n",
    "# arr\n",
    "import cv2\n",
    "image = cv2.imread(tri[0])\n",
    "image = np.fliplr(image)\n",
    "# plt.imshow(image)\n",
    "imgs = []\n",
    "imgs.append(img_A)\n",
    "imgs.append(img_B)\n",
    "imgs = np.array(imgs)\n",
    "imgs = np.array(imgs)/127.5 - 1.\n",
    "imgs.shape\n",
    "# np.fliplr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "resolution = (128,128)\n",
    "def load_images(dataset,batch_size):\n",
    "    chance = np.random.random()\n",
    "    path = glob('cityscapes/train/*')\n",
    "    images = np.random.choice(path, size=batch_size)\n",
    "    img_real = []\n",
    "    img_labelled = []\n",
    "    \n",
    "    for imagepath in images:\n",
    "        img = cv2.imread(imagepath)\n",
    "        width = img.shape[1]\n",
    "        width = width//2\n",
    "        real_img, labelled_img = img[:, width:, :],img[:, :width, :]\n",
    "        real_img,labelled_img = transform.resize(real_img, resolution),transform.resize(labelled_img, resolution)\n",
    "        if (chance<0.5):\n",
    "            real_img = np.fliplr(real_img)\n",
    "            labelled_img = np.fliplr(labelled_img)\n",
    "        img_real.append(real_img)\n",
    "        img_labelled.append(labelled_img)\n",
    "    return np.array(img_labelled),np.array(img_real)\n",
    "\n",
    "def train_load_images(dataset,batch_size):\n",
    "    chance = np.random.random()\n",
    "    path = glob('cityscapes/train/*')\n",
    "    images = np.random.choice(path, size=batch_size)\n",
    "    img_real = []\n",
    "    img_labelled = []\n",
    "    \n",
    "    for imagepath in images:\n",
    "        img = cv2.imread(imagepath)\n",
    "        width = img.shape[1]\n",
    "        width = width//2\n",
    "        real_img, labelled_img = img[:, width:, :],img[:, :width, :]\n",
    "        real_img,labelled_img = transform.resize(real_img, resolution),transform.resize(labelled_img, resolution)\n",
    "        if (chance<0.5):\n",
    "            real_img = np.fliplr(real_img)\n",
    "            labelled_img = np.fliplr(labelled_img)\n",
    "        img_real.append(real_img)\n",
    "        img_labelled.append(labelled_img)\n",
    "    return np.array(img_labelled),np.array(img_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hy-51JJEKuOI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Works\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_165 (Concatenate)   (None, 128, 128, 6)  0           input_17[0][0]                   \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_629 (Conv2D)             (None, 64, 64, 64)   6208        concatenate_165[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_443 (LeakyReLU)     (None, 64, 64, 64)   0           conv2d_629[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_630 (Conv2D)             (None, 32, 32, 128)  131200      leaky_re_lu_443[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_444 (LeakyReLU)     (None, 32, 32, 128)  0           conv2d_630[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_631 (Conv2D)             (None, 16, 16, 256)  524544      leaky_re_lu_444[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_445 (LeakyReLU)     (None, 16, 16, 256)  0           conv2d_631[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_632 (Conv2D)             (None, 8, 8, 512)    2097664     leaky_re_lu_445[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_446 (LeakyReLU)     (None, 8, 8, 512)    0           conv2d_632[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_633 (Conv2D)             (None, 8, 8, 1)      8193        leaky_re_lu_446[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 2,767,809\n",
      "Trainable params: 2,767,809\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input shape\n",
    "img_rows = 128\n",
    "img_cols = 128\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = int(img_rows / 2**4)\n",
    "disc_patch = (patch, patch, 1)\n",
    "\n",
    "# Number of filters in the first layer of G and D\n",
    "filters_gen = 64\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "def generator():\n",
    "    layer0 = Input(shape=img_shape,name=\"input\")\n",
    "    layer1 = Conv2D(filters_gen, kernel_size=4,strides = 2,activation = LeakyReLU(alpha=0.2),padding='same')(layer0)\n",
    "    layer2 = Conv2D(filters_gen*2, kernel_size=4,strides = 2,activation = LeakyReLU(alpha=0.2),padding='same')(layer1)\n",
    "    layer3 = Conv2D(filters_gen*4, kernel_size=4,strides = 2,activation = LeakyReLU(alpha=0.2),padding='same')(layer2)\n",
    "    layer4 = Conv2D(filters_gen*8, kernel_size=4,strides = 2,activation = LeakyReLU(alpha=0.2),padding='same')(layer3)\n",
    "    layer5 = Conv2D(filters_gen*8, kernel_size=4,strides = 2,activation = LeakyReLU(alpha=0.2),padding='same')(layer4)\n",
    "    layer6 = Conv2D(filters_gen*8, kernel_size=4,strides = 2,activation = LeakyReLU(alpha=0.2),padding='same')(layer5)\n",
    "    layer7 = Conv2D(filters_gen*8, kernel_size=2,strides = 2,activation = LeakyReLU(alpha=0.2),padding='same')(layer6)\n",
    "    \n",
    "    def deconv2d(prev_layer, skip_input, filters):\n",
    "        temp = UpSampling2D(size=2)(prev_layer)\n",
    "        temp = Conv2D(filters, kernel_size=4, strides=1, padding='same', activation='relu')(temp)\n",
    "        temp = BatchNormalization(momentum=0.6)(temp)\n",
    "        temp = Concatenate()([temp, skip_input])\n",
    "        return temp\n",
    "    \n",
    "    u_layer1 = deconv2d(layer7, layer6, filters_gen*8)\n",
    "    u_layer2 = deconv2d(u_layer1, layer5, filters_gen*8)\n",
    "    u_layer3 = deconv2d(u_layer2, layer4, filters_gen*8)\n",
    "    u_layer4 = deconv2d(u_layer3, layer3, filters_gen*4)\n",
    "    u_layer5 = deconv2d(u_layer4, layer2, filters_gen*2)\n",
    "    u_layer6 = deconv2d(u_layer5, layer1, filters_gen)\n",
    "    u_layer7 = UpSampling2D(size=2)(u_layer6)\n",
    "    u_layer0 = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u_layer7)\n",
    "    return Model(layer0,u_layer0)\n",
    "hell = generator()\n",
    "\n",
    "def discrimantor():\n",
    "    def d_layer(prev_layer, filters):\n",
    "        temp = Conv2D(filters, kernel_size=4, strides=2, padding='same')(prev_layer)\n",
    "        temp = LeakyReLU(alpha=0.2)(temp)\n",
    "        return temp\n",
    "\n",
    "    layer0_A = Input(shape=img_shape)\n",
    "    layer0_B = Input(shape=img_shape)\n",
    "    combined_input = Concatenate(axis=-1)([layer0_A, layer0_B])\n",
    "    layer1 = d_layer(combined_input, filters_gen)\n",
    "    layer2 = d_layer(layer1, filters_gen*2)\n",
    "    layer3 = d_layer(layer2, filters_gen*4)\n",
    "    layer4 = d_layer(layer3, filters_gen*8)\n",
    "    layer5 = Conv2D(1, kernel_size=4, strides=1, padding='same')(layer4)\n",
    "    return Model([layer0_A, layer0_B], layer5)\n",
    "\n",
    "heaven = discrimantor()\n",
    "heaven.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atfrZqe8K3JF"
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization(momentum=0.8)(u)\n",
    "            u = Concatenate()([u, skip_input]) #skip connection\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, gf, bn=False)\n",
    "        d2 = conv2d(d1, gf*2)\n",
    "        d3 = conv2d(d2, gf*4)\n",
    "        d4 = conv2d(d3, gf*8)\n",
    "        d5 = conv2d(d4, gf*8)\n",
    "        d6 = conv2d(d5, gf*8)\n",
    "        d7 = conv2d(d6, gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d7, d6, gf*8)\n",
    "        u2 = deconv2d(u1, d5, gf*8)\n",
    "        u3 = deconv2d(u2, d4, gf*8)\n",
    "        u4 = deconv2d(u3, d3, gf*4)\n",
    "        u5 = deconv2d(u4, d2, gf*2)\n",
    "        u6 = deconv2d(u5, d1, gf)\n",
    "\n",
    "        u7 = UpSampling2D(size=2)(u6)\n",
    "        output_img = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "      \n",
    "def build_discriminator():\n",
    "        # a small function to make one layer of the discriminator\n",
    "        def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        img_A = Input(shape=img_shape)\n",
    "        img_B = Input(shape=img_shape)\n",
    "\n",
    "        # Concatenate image and conditioning image by channels to produce input\n",
    "        combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "\n",
    "        d1 = d_layer(combined_imgs, df, bn=False)\n",
    "        d2 = d_layer(d1, df*2)\n",
    "        d3 = d_layer(d2, df*4)\n",
    "        d4 = d_layer(d3, df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model([img_A, img_B], validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "EdoAehpUMXtM",
    "outputId": "d13506e1-a9bd-4cfe-b94c-a49cfb285c67"
   },
   "outputs": [],
   "source": [
    "# Input shape\n",
    "img_rows = 128\n",
    "img_cols = 128\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = int(img_rows / 2**4)\n",
    "disc_patch = (patch, patch, 1)\n",
    "\n",
    "# Number of filters in the first layer of G and D\n",
    "gf = 64\n",
    "df = 64\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# Input images and their conditioning images\n",
    "img_A = Input(shape=img_shape,name='img_A')\n",
    "img_B = Input(shape=img_shape)\n",
    "\n",
    "# By conditioning on B generate a fake version of A\n",
    "fake_A = generator(img_B)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Discriminators determines validity of translated images / condition pairs\n",
    "valid = discriminator([fake_A, img_B])\n",
    "\n",
    "combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "combined.compile(loss=['mse', 'mae'],\n",
    "                              loss_weights=[1, 100],\n",
    "                              optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cy1rvvRgMuRv"
   },
   "outputs": [],
   "source": [
    "def show_images( dataset_name,epoch, batch_i):\n",
    "        \n",
    "        r, c = 3, 3\n",
    "\n",
    "        imgs_A, imgs_B = load_data(dataset_name,batch_size=3, is_val=True)\n",
    "        fake_A = generator.predict(imgs_B)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Input', 'Output', 'Ground Truth']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[i])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "def train( dataset_name,epochs, batch_size=1, show_interval=10):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + disc_patch)\n",
    "        fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(dataset_name,batch_size)):\n",
    "\n",
    "                \n",
    "                #  Train Discriminator\n",
    "                \n",
    "\n",
    "                # Condition on B and generate a translated version\n",
    "                fake_A = generator.predict(imgs_B)\n",
    "\n",
    "                # Train the discriminators (original images = real / generated = Fake)\n",
    "                d_loss_real = discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
    "                d_loss_fake = discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "               \n",
    "                #  Train Generator\n",
    "                g_loss = combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "                \n",
    "            # Plot the progress\n",
    "            if epoch%10==0:\n",
    "                  print (\"[Epoch %d/%d]  [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, epochs,\n",
    "                                                                        \n",
    "                                                                        d_loss[0], 100*d_loss[1],\n",
    "                                                                        g_loss[0],\n",
    "                                                                        elapsed_time))\n",
    "            # If at show interval => show generated image samples\n",
    "            if epoch % show_interval == 0:\n",
    "                    show_images(dataset_name,epoch, batch_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OEDA0bytNnNj",
    "outputId": "f1ac297e-741d-4696-e5a2-c3c841c6d08e"
   },
   "outputs": [],
   "source": [
    "train(\"cityscapes\",epochs=50, batch_size=32, show_interval=10)\n",
    "# print(os.listdir(\"cityscapes\"))\n",
    "# !mkdir input\n",
    "# patch = int(img_rows / 2**4)\n",
    "# disc_patch = (patch, patch, 1)\n",
    "# disc_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "pix2pixGAN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
