{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DNkuyu-RJdKW",
    "outputId": "c25a5b72-91c8-4e4c-f5d1-77b7b31f925d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "from imageio import imread\n",
    "# print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cityscapes/train/1258.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 256, 256, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = glob('cityscapes/train/*')\n",
    "tri = np.random.choice(path, size=1)\n",
    "print(tri[0])\n",
    "img = imread(tri[0])\n",
    "h, w, _ = img.shape\n",
    "# img.shape\n",
    "# _w\n",
    "_w = int(w/2)\n",
    "\n",
    "img_A, img_B = img[:, :_w, :], img[:, _w:, :]\n",
    "# img\n",
    "arr = np.array([[1,2,3,4],[5,6,7,8]])\n",
    "# img_A.shape\n",
    "# print(arr[:1,:])\n",
    "# print(arr[:])\n",
    "# arr.shape\n",
    "# arr = transform.resize(arr, (1,2))\n",
    "# arr\n",
    "import cv2\n",
    "image = cv2.imread(tri[0])\n",
    "image = np.fliplr(image)\n",
    "# plt.imshow(image)\n",
    "imgs = []\n",
    "imgs.append(img_A)\n",
    "imgs.append(img_B)\n",
    "imgs = np.array(imgs)\n",
    "imgs = np.array(imgs)/127.5 - 1.\n",
    "imgs.shape\n",
    "# np.fliplr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "resolution = (128,128)\n",
    "def load_images(dataset,batch_size):\n",
    "    chance = np.random.random()\n",
    "    path = glob('cityscapes/train/*')\n",
    "    images = np.random.choice(path, size=batch_size)\n",
    "    img_real = []\n",
    "    img_labelled = []\n",
    "    \n",
    "    for imagepath in images:\n",
    "        img = cv2.imread(imagepath)\n",
    "        width = img.shape[1]\n",
    "        width = width//2\n",
    "        real_img, labelled_img = img[:, width:, :],img[:, :width, :]\n",
    "        real_img,labelled_img = transform.resize(real_img, resolution),transform.resize(labelled_img, resolution)\n",
    "        if (chance<0.5):\n",
    "            real_img = np.fliplr(real_img)\n",
    "            labelled_img = np.fliplr(labelled_img)\n",
    "        img_real.append(real_img)\n",
    "        img_labelled.append(labelled_img)\n",
    "    return np.array(img_labelled),np.array(img_real)\n",
    "\n",
    "def train_load_images(dataset,batch_size):\n",
    "    chance = np.random.random()\n",
    "    path = glob('cityscapes/train/*')\n",
    "    images = np.random.choice(path, size=batch_size)\n",
    "    img_real = []\n",
    "    img_labelled = []\n",
    "    \n",
    "    for imagepath in images:\n",
    "        img = cv2.imread(imagepath)\n",
    "        width = img.shape[1]\n",
    "        width = width//2\n",
    "        real_img, labelled_img = img[:, width:, :],img[:, :width, :]\n",
    "        real_img,labelled_img = transform.resize(real_img, resolution),transform.resize(labelled_img, resolution)\n",
    "        if (chance<0.5):\n",
    "            real_img = np.fliplr(real_img)\n",
    "            labelled_img = np.fliplr(labelled_img)\n",
    "        img_real.append(real_img)\n",
    "        img_labelled.append(labelled_img)\n",
    "    return np.array(img_labelled),np.array(img_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hy-51JJEKuOI"
   },
   "outputs": [],
   "source": [
    "# Input shape\n",
    "img_rows = 128\n",
    "img_cols = 128\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = int(img_rows / 2**4)\n",
    "disc_patch = (patch, patch, 1)\n",
    "\n",
    "# Number of filters in the first layer of G and D\n",
    "filters_gen = 64\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "def new_generator():\n",
    "    layer0 = Input(shape=img_shape,name=\"input\")\n",
    "    layer1 = Conv2D(filters_gen, kernel_size=4,strides = 2,activation = LeakyReLU(alpha=0.2),padding='same')(layer0)\n",
    "    layer2 = Conv2D(filters_gen*2, kernel_size=4,strides = 2,activation = LeakyReLU(alpha=0.2),padding='same')(layer1)\n",
    "    layer3 = Conv2D(filters_gen*4, kernel_size=4,strides = 2,activation = LeakyReLU(alpha=0.2),padding='same')(layer2)\n",
    "    layer4 = Conv2D(filters_gen*8, kernel_size=4,strides = 2,activation = LeakyReLU(alpha=0.2),padding='same')(layer3)\n",
    "    layer5 = Conv2D(filters_gen*8, kernel_size=4,strides = 2,activation = LeakyReLU(alpha=0.2),padding='same')(layer4)\n",
    "    layer6 = Conv2D(filters_gen*8, kernel_size=4,strides = 2,activation = LeakyReLU(alpha=0.2),padding='same')(layer5)\n",
    "    layer7 = Conv2D(filters_gen*8, kernel_size=2,strides = 2,activation = LeakyReLU(alpha=0.2),padding='same')(layer6)\n",
    "    \n",
    "    def deconv2d(prev_layer, skip_input, filters):\n",
    "        temp = UpSampling2D(size=2)(prev_layer)\n",
    "        temp = Conv2D(filters, kernel_size=4, strides=1, padding='same', activation='relu')(temp)\n",
    "        temp = BatchNormalization(momentum=0.6)(temp)\n",
    "        temp = Concatenate()([temp, skip_input])\n",
    "        return temp\n",
    "    \n",
    "    u_layer1 = deconv2d(layer7, layer6, filters_gen*8)\n",
    "    u_layer2 = deconv2d(u_layer1, layer5, filters_gen*8)\n",
    "    u_layer3 = deconv2d(u_layer2, layer4, filters_gen*8)\n",
    "    u_layer4 = deconv2d(u_layer3, layer3, filters_gen*4)\n",
    "    u_layer5 = deconv2d(u_layer4, layer2, filters_gen*2)\n",
    "    u_layer6 = deconv2d(u_layer5, layer1, filters_gen)\n",
    "    u_layer7 = UpSampling2D(size=2)(u_layer6)\n",
    "    u_layer0 = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u_layer7)\n",
    "    return Model(layer0,u_layer0,name=\"Generator\")\n",
    "# hell = new_generator()\n",
    "# hell.summary()\n",
    "\n",
    "def new_discriminator():\n",
    "    def d_layer(prev_layer, filters):\n",
    "        temp = Conv2D(filters, kernel_size=4, strides=2, padding='same')(prev_layer)\n",
    "        temp = LeakyReLU(alpha=0.2)(temp)\n",
    "        return temp\n",
    "\n",
    "    layer0_A = Input(shape=img_shape)\n",
    "    layer0_B = Input(shape=img_shape)\n",
    "    combined_input = Concatenate(axis=-1)([layer0_A, layer0_B])\n",
    "    layer1 = d_layer(combined_input, filters_gen)\n",
    "    layer2 = d_layer(layer1, filters_gen*2)\n",
    "    layer3 = d_layer(layer2, filters_gen*4)\n",
    "    layer4 = d_layer(layer3, filters_gen*8)\n",
    "    layer5 = Conv2D(1, kernel_size=4, strides=1, padding='same')(layer4)\n",
    "    return Model([layer0_A, layer0_B], layer5,name=\"Discriminator\")\n",
    "\n",
    "# heaven = new_discriminator()\n",
    "# heaven.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_45 (InputLayer)           (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_46 (InputLayer)           (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_85 (Concatenate)    (None, 128, 128, 6)  0           input_45[0][0]                   \n",
      "                                                                 input_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_229 (Conv2D)             (None, 64, 64, 64)   6208        concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_133 (LeakyReLU)     (None, 64, 64, 64)   0           conv2d_229[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_230 (Conv2D)             (None, 32, 32, 128)  131200      leaky_re_lu_133[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_134 (LeakyReLU)     (None, 32, 32, 128)  0           conv2d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)             (None, 16, 16, 256)  524544      leaky_re_lu_134[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_135 (LeakyReLU)     (None, 16, 16, 256)  0           conv2d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)             (None, 8, 8, 512)    2097664     leaky_re_lu_135[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_136 (LeakyReLU)     (None, 8, 8, 512)    0           conv2d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_233 (Conv2D)             (None, 8, 8, 1)      8193        leaky_re_lu_136[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 5,535,618\n",
      "Trainable params: 2,767,809\n",
      "Non-trainable params: 2,767,809\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. Two images as input.\n",
    "2. One image goes to generator\n",
    "3. Two images go to the discriminator as input\n",
    "4. We freeze the discriminator weights\n",
    "5. Create GAN with both models. Inputs as earlier and outputs as earlier\n",
    "'''\n",
    "d = new_discriminator()\n",
    "d.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "#Since the model is being compiled already here, the flag later of trainable = False does not affect it.\n",
    "\n",
    "g = new_generator()\n",
    "\n",
    "A = Input(shape = img_shape)\n",
    "B = Input(shape = img_shape)\n",
    "\n",
    "fake_image_B = g(B)\n",
    "validity = d([fake_image_B,B])\n",
    "\n",
    "#THIS ONLY STOPS THE TRAINING OF THE DISCRIMINATOR IN THE GAN. IT WILL STILL TRAIN WHEN INVOKED AS JUST D.\n",
    "d.trainable = False\n",
    "\n",
    "gan = Model(inputs=[A,B],outputs = [validity,fake_image_B],name=\"hey\")\n",
    "\n",
    "gan.compile(loss=['mse', 'mae'],\n",
    "                              loss_weights=[1, 100],\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atfrZqe8K3JF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "EdoAehpUMXtM",
    "outputId": "d13506e1-a9bd-4cfe-b94c-a49cfb285c67"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_discriminator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fd24dfc52f3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Build and compile the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m discriminator.compile(loss='mse',\n\u001b[1;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_discriminator' is not defined"
     ]
    }
   ],
   "source": [
    "# Input shape\n",
    "img_rows = 128\n",
    "img_cols = 128\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = int(img_rows / 2**4)\n",
    "disc_patch = (patch, patch, 1)\n",
    "\n",
    "# Number of filters in the first layer of G and D\n",
    "gf = 64\n",
    "df = 64\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# Input images and their conditioning images\n",
    "img_A = Input(shape=img_shape,name='img_A')\n",
    "img_B = Input(shape=img_shape)\n",
    "\n",
    "# By conditioning on B generate a fake version of A\n",
    "fake_A = generator(img_B)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Discriminators determines validity of translated images / condition pairs\n",
    "valid = discriminator([fake_A, img_B])\n",
    "\n",
    "combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "combined.compile(loss=['mse', 'mae'],\n",
    "                              loss_weights=[1, 100],\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "valid = np.ones((batch_size,) + disc_patch)\n",
    "print(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cy1rvvRgMuRv"
   },
   "outputs": [],
   "source": [
    "def show_images( dataset_name,epoch, batch_i):\n",
    "        \n",
    "        r, c = 3, 3\n",
    "\n",
    "        imgs_A, imgs_B = load_data(dataset_name,batch_size=3, is_val=True)\n",
    "        fake_A = generator.predict(imgs_B)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Input', 'Output', 'Ground Truth']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[i])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "def train( dataset_name,epochs, batch_size=1, show_interval=10):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + disc_patch)\n",
    "        fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(dataset_name,batch_size)):\n",
    "\n",
    "                \n",
    "                #  Train Discriminator\n",
    "                \n",
    "\n",
    "                # Condition on B and generate a translated version\n",
    "                fake_A = generator.predict(imgs_B)\n",
    "\n",
    "                # Train the discriminators (original images = real / generated = Fake)\n",
    "                d_loss_real = discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
    "                d_loss_fake = discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "               \n",
    "                #  Train GAN\n",
    "                g_loss = combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "                \n",
    "            # Plot the progress\n",
    "            if epoch%10==0:\n",
    "                  print (\"[Epoch %d/%d]  [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, epochs,\n",
    "                                                                        \n",
    "                                                                        d_loss[0], 100*d_loss[1],\n",
    "                                                                        g_loss[0],\n",
    "                                                                        elapsed_time))\n",
    "            # If at show interval => show generated image samples\n",
    "            if epoch % show_interval == 0:\n",
    "                    show_images(dataset_name,epoch, batch_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OEDA0bytNnNj",
    "outputId": "f1ac297e-741d-4696-e5a2-c3c841c6d08e"
   },
   "outputs": [],
   "source": [
    "train(\"cityscapes\",epochs=50, batch_size=32, show_interval=10)\n",
    "# print(os.listdir(\"cityscapes\"))\n",
    "# !mkdir input\n",
    "# patch = int(img_rows / 2**4)\n",
    "# disc_patch = (patch, patch, 1)\n",
    "# disc_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "pix2pixGAN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
